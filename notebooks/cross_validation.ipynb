{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../modules'))\n",
    "from best_pipelines import models_to_cv\n",
    "from cross_validation import cross_validation\n",
    "from preproc import PhishingDatasetPreproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = PhishingDatasetPreproc()\n",
    "dataset, X, y = prep.basic_operations()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipelines, ensembles = models_to_cv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model KNN\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0        KNN  fold_0  0.961027  0.965066  [[356, 16], [16, 442]]  0.965066   \n",
      "1        KNN  fold_1  0.955363  0.967177  [[351, 21], [15, 442]]  0.960870   \n",
      "2        KNN  fold_2  0.963521  0.978118  [[353, 19], [10, 447]]  0.968581   \n",
      "3        KNN  fold_3  0.956957  0.964989  [[353, 19], [16, 441]]  0.961832   \n",
      "4        KNN  fold_4  0.963177  0.969365  [[356, 16], [14, 443]]  0.967249   \n",
      "5        KNN  fold_5  0.953975  0.967249  [[349, 22], [15, 443]]  0.959913   \n",
      "6        KNN  fold_6  0.966496  0.986900   [[351, 20], [6, 452]]  0.972043   \n",
      "7        KNN  fold_7  0.969123  0.975983  [[357, 14], [11, 447]]  0.972797   \n",
      "8        KNN  fold_8  0.966940  0.971616  [[357, 14], [13, 445]]  0.970556   \n",
      "9        KNN  fold_9  0.966428  0.975983  [[355, 16], [11, 447]]  0.970684   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0       0.150595        0.258595    0.038554        0.991020       0.961446   \n",
      "1       0.216386        0.258142    0.043426        0.989815       0.956574   \n",
      "2       0.157795        0.208168    0.034982        0.989681       0.965018   \n",
      "3       0.150365        0.200379    0.042220        0.990753       0.957780   \n",
      "4       0.158504        0.225396    0.036188        0.989815       0.963812   \n",
      "5       0.145381        0.208846    0.044632        0.991423       0.955368   \n",
      "6       0.150019        0.207699    0.031363        0.990217       0.968637   \n",
      "7       0.158299        0.207437    0.030157        0.989011       0.969843   \n",
      "8       0.247245        0.224001    0.032569        0.990753       0.967431   \n",
      "9       0.133836        0.233562    0.032569        0.989413       0.967431   \n",
      "\n",
      "   precision  \n",
      "0   0.965066  \n",
      "1   0.954644  \n",
      "2   0.959227  \n",
      "3   0.958696  \n",
      "4   0.965142  \n",
      "5   0.952688  \n",
      "6   0.957627  \n",
      "7   0.969631  \n",
      "8   0.969499  \n",
      "9   0.965443  \n",
      "Model LVQ\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "   model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0         KNN  fold_0  0.961027  0.965066  [[356, 16], [16, 442]]  0.965066   \n",
      "1         KNN  fold_1  0.955363  0.967177  [[351, 21], [15, 442]]  0.960870   \n",
      "2         KNN  fold_2  0.963521  0.978118  [[353, 19], [10, 447]]  0.968581   \n",
      "3         KNN  fold_3  0.956957  0.964989  [[353, 19], [16, 441]]  0.961832   \n",
      "4         KNN  fold_4  0.963177  0.969365  [[356, 16], [14, 443]]  0.967249   \n",
      "5         KNN  fold_5  0.953975  0.967249  [[349, 22], [15, 443]]  0.959913   \n",
      "6         KNN  fold_6  0.966496  0.986900   [[351, 20], [6, 452]]  0.972043   \n",
      "7         KNN  fold_7  0.969123  0.975983  [[357, 14], [11, 447]]  0.972797   \n",
      "8         KNN  fold_8  0.966940  0.971616  [[357, 14], [13, 445]]  0.970556   \n",
      "9         KNN  fold_9  0.966428  0.975983  [[355, 16], [11, 447]]  0.970684   \n",
      "10        LVQ  fold_0  0.893494  0.921397  [[322, 50], [36, 422]]  0.907527   \n",
      "11        LVQ  fold_1  0.904005  0.934354  [[325, 47], [30, 427]]  0.917293   \n",
      "12        LVQ  fold_2  0.863777  0.945295  [[291, 81], [25, 432]]  0.890722   \n",
      "13        LVQ  fold_3  0.876718  0.949672  [[299, 73], [23, 434]]  0.900415   \n",
      "14        LVQ  fold_4  0.903411  0.927790  [[327, 45], [33, 424]]  0.915767   \n",
      "15        LVQ  fold_5  0.886198  0.947598  [[306, 65], [24, 434]]  0.907001   \n",
      "16        LVQ  fold_6  0.889729  0.951965  [[307, 64], [22, 436]]  0.910230   \n",
      "17        LVQ  fold_7  0.899863  0.934498  [[321, 50], [30, 428]]  0.914530   \n",
      "18        LVQ  fold_8  0.906158  0.949782  [[320, 51], [23, 435]]  0.921610   \n",
      "19        LVQ  fold_9  0.907505  0.949782  [[321, 50], [23, 435]]  0.922587   \n",
      "\n",
      "    training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0        0.150595        0.258595    0.038554        0.991020       0.961446   \n",
      "1        0.216386        0.258142    0.043426        0.989815       0.956574   \n",
      "2        0.157795        0.208168    0.034982        0.989681       0.965018   \n",
      "3        0.150365        0.200379    0.042220        0.990753       0.957780   \n",
      "4        0.158504        0.225396    0.036188        0.989815       0.963812   \n",
      "5        0.145381        0.208846    0.044632        0.991423       0.955368   \n",
      "6        0.150019        0.207699    0.031363        0.990217       0.968637   \n",
      "7        0.158299        0.207437    0.030157        0.989011       0.969843   \n",
      "8        0.247245        0.224001    0.032569        0.990753       0.967431   \n",
      "9        0.133836        0.233562    0.032569        0.989413       0.967431   \n",
      "10      18.022866        0.103858    0.103614        0.901622       0.896386   \n",
      "11      18.671879        0.095556    0.092883        0.900563       0.907117   \n",
      "12      18.596905        0.074591    0.127865        0.891450       0.872135   \n",
      "13      31.233515        0.103505    0.115802        0.883543       0.884198   \n",
      "14      20.103187        0.303124    0.094089        0.904985       0.905911   \n",
      "15      65.548773        0.300754    0.107358        0.903109       0.892642   \n",
      "16      66.400597        0.329551    0.103739        0.894532       0.896261   \n",
      "17      67.791979        0.109562    0.096502        0.900965       0.903498   \n",
      "18      40.109460        0.294844    0.089264        0.895202       0.910736   \n",
      "19      62.336055        0.276352    0.088058        0.902975       0.911942   \n",
      "\n",
      "    precision  \n",
      "0    0.965066  \n",
      "1    0.954644  \n",
      "2    0.959227  \n",
      "3    0.958696  \n",
      "4    0.965142  \n",
      "5    0.952688  \n",
      "6    0.957627  \n",
      "7    0.969631  \n",
      "8    0.969499  \n",
      "9    0.965443  \n",
      "10   0.894068  \n",
      "11   0.900844  \n",
      "12   0.842105  \n",
      "13   0.856016  \n",
      "14   0.904051  \n",
      "15   0.869739  \n",
      "16   0.872000  \n",
      "17   0.895397  \n",
      "18   0.895062  \n",
      "19   0.896907  \n",
      "Model DTR\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "   model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0         KNN  fold_0  0.961027  0.965066  [[356, 16], [16, 442]]  0.965066   \n",
      "1         KNN  fold_1  0.955363  0.967177  [[351, 21], [15, 442]]  0.960870   \n",
      "2         KNN  fold_2  0.963521  0.978118  [[353, 19], [10, 447]]  0.968581   \n",
      "3         KNN  fold_3  0.956957  0.964989  [[353, 19], [16, 441]]  0.961832   \n",
      "4         KNN  fold_4  0.963177  0.969365  [[356, 16], [14, 443]]  0.967249   \n",
      "5         KNN  fold_5  0.953975  0.967249  [[349, 22], [15, 443]]  0.959913   \n",
      "6         KNN  fold_6  0.966496  0.986900   [[351, 20], [6, 452]]  0.972043   \n",
      "7         KNN  fold_7  0.969123  0.975983  [[357, 14], [11, 447]]  0.972797   \n",
      "8         KNN  fold_8  0.966940  0.971616  [[357, 14], [13, 445]]  0.970556   \n",
      "9         KNN  fold_9  0.966428  0.975983  [[355, 16], [11, 447]]  0.970684   \n",
      "10        LVQ  fold_0  0.893494  0.921397  [[322, 50], [36, 422]]  0.907527   \n",
      "11        LVQ  fold_1  0.904005  0.934354  [[325, 47], [30, 427]]  0.917293   \n",
      "12        LVQ  fold_2  0.863777  0.945295  [[291, 81], [25, 432]]  0.890722   \n",
      "13        LVQ  fold_3  0.876718  0.949672  [[299, 73], [23, 434]]  0.900415   \n",
      "14        LVQ  fold_4  0.903411  0.927790  [[327, 45], [33, 424]]  0.915767   \n",
      "15        LVQ  fold_5  0.886198  0.947598  [[306, 65], [24, 434]]  0.907001   \n",
      "16        LVQ  fold_6  0.889729  0.951965  [[307, 64], [22, 436]]  0.910230   \n",
      "17        LVQ  fold_7  0.899863  0.934498  [[321, 50], [30, 428]]  0.914530   \n",
      "18        LVQ  fold_8  0.906158  0.949782  [[320, 51], [23, 435]]  0.921610   \n",
      "19        LVQ  fold_9  0.907505  0.949782  [[321, 50], [23, 435]]  0.922587   \n",
      "20        DTR  fold_0  0.935490  0.965066  [[337, 35], [16, 442]]  0.945455   \n",
      "21        DTR  fold_1  0.933169  0.949672  [[341, 31], [23, 434]]  0.941432   \n",
      "22        DTR  fold_2  0.926449  0.949672  [[336, 36], [23, 434]]  0.936354   \n",
      "23        DTR  fold_3  0.932763  0.964989  [[335, 37], [16, 441]]  0.943316   \n",
      "24        DTR  fold_4  0.937796  0.956236  [[342, 30], [20, 437]]  0.945887   \n",
      "25        DTR  fold_5  0.926765  0.969432  [[328, 43], [14, 444]]  0.939683   \n",
      "26        DTR  fold_6  0.927789  0.960699  [[332, 39], [18, 440]]  0.939168   \n",
      "27        DTR  fold_7  0.927141  0.943231  [[338, 33], [26, 432]]  0.936078   \n",
      "28        DTR  fold_8  0.936319  0.945415  [[344, 27], [25, 433]]  0.943355   \n",
      "29        DTR  fold_9  0.934783  0.958515  [[338, 33], [19, 439]]  0.944086   \n",
      "\n",
      "    training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0        0.150595        0.258595    0.038554        0.991020       0.961446   \n",
      "1        0.216386        0.258142    0.043426        0.989815       0.956574   \n",
      "2        0.157795        0.208168    0.034982        0.989681       0.965018   \n",
      "3        0.150365        0.200379    0.042220        0.990753       0.957780   \n",
      "4        0.158504        0.225396    0.036188        0.989815       0.963812   \n",
      "5        0.145381        0.208846    0.044632        0.991423       0.955368   \n",
      "6        0.150019        0.207699    0.031363        0.990217       0.968637   \n",
      "7        0.158299        0.207437    0.030157        0.989011       0.969843   \n",
      "8        0.247245        0.224001    0.032569        0.990753       0.967431   \n",
      "9        0.133836        0.233562    0.032569        0.989413       0.967431   \n",
      "10      18.022866        0.103858    0.103614        0.901622       0.896386   \n",
      "11      18.671879        0.095556    0.092883        0.900563       0.907117   \n",
      "12      18.596905        0.074591    0.127865        0.891450       0.872135   \n",
      "13      31.233515        0.103505    0.115802        0.883543       0.884198   \n",
      "14      20.103187        0.303124    0.094089        0.904985       0.905911   \n",
      "15      65.548773        0.300754    0.107358        0.903109       0.892642   \n",
      "16      66.400597        0.329551    0.103739        0.894532       0.896261   \n",
      "17      67.791979        0.109562    0.096502        0.900965       0.903498   \n",
      "18      40.109460        0.294844    0.089264        0.895202       0.910736   \n",
      "19      62.336055        0.276352    0.088058        0.902975       0.911942   \n",
      "20       1.338725        0.046830    0.061446        0.937408       0.938554   \n",
      "21       1.257717        0.048533    0.065139        0.938354       0.934861   \n",
      "22       1.304446        0.046874    0.071170        0.937148       0.928830   \n",
      "23       1.259512        0.051344    0.063932        0.937818       0.936068   \n",
      "24       1.242452        0.047267    0.060314        0.935674       0.939686   \n",
      "25       1.305387        0.041892    0.068758        0.938622       0.931242   \n",
      "26       1.244323        0.047341    0.068758        0.938354       0.931242   \n",
      "27       1.238985        0.031255    0.071170        0.937282       0.928830   \n",
      "28       1.259360        0.036992    0.062726        0.933798       0.937274   \n",
      "29       1.287800        0.047441    0.062726        0.937416       0.937274   \n",
      "\n",
      "    precision  \n",
      "0    0.965066  \n",
      "1    0.954644  \n",
      "2    0.959227  \n",
      "3    0.958696  \n",
      "4    0.965142  \n",
      "5    0.952688  \n",
      "6    0.957627  \n",
      "7    0.969631  \n",
      "8    0.969499  \n",
      "9    0.965443  \n",
      "10   0.894068  \n",
      "11   0.900844  \n",
      "12   0.842105  \n",
      "13   0.856016  \n",
      "14   0.904051  \n",
      "15   0.869739  \n",
      "16   0.872000  \n",
      "17   0.895397  \n",
      "18   0.895062  \n",
      "19   0.896907  \n",
      "20   0.926625  \n",
      "21   0.933333  \n",
      "22   0.923404  \n",
      "23   0.922594  \n",
      "24   0.935760  \n",
      "25   0.911704  \n",
      "26   0.918580  \n",
      "27   0.929032  \n",
      "28   0.941304  \n",
      "29   0.930085  \n",
      "Model SVM\n",
      "Fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Metrics: \n",
      "   model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0         KNN  fold_0  0.961027  0.965066  [[356, 16], [16, 442]]  0.965066   \n",
      "1         KNN  fold_1  0.955363  0.967177  [[351, 21], [15, 442]]  0.960870   \n",
      "2         KNN  fold_2  0.963521  0.978118  [[353, 19], [10, 447]]  0.968581   \n",
      "3         KNN  fold_3  0.956957  0.964989  [[353, 19], [16, 441]]  0.961832   \n",
      "4         KNN  fold_4  0.963177  0.969365  [[356, 16], [14, 443]]  0.967249   \n",
      "5         KNN  fold_5  0.953975  0.967249  [[349, 22], [15, 443]]  0.959913   \n",
      "6         KNN  fold_6  0.966496  0.986900   [[351, 20], [6, 452]]  0.972043   \n",
      "7         KNN  fold_7  0.969123  0.975983  [[357, 14], [11, 447]]  0.972797   \n",
      "8         KNN  fold_8  0.966940  0.971616  [[357, 14], [13, 445]]  0.970556   \n",
      "9         KNN  fold_9  0.966428  0.975983  [[355, 16], [11, 447]]  0.970684   \n",
      "10        LVQ  fold_0  0.893494  0.921397  [[322, 50], [36, 422]]  0.907527   \n",
      "11        LVQ  fold_1  0.904005  0.934354  [[325, 47], [30, 427]]  0.917293   \n",
      "12        LVQ  fold_2  0.863777  0.945295  [[291, 81], [25, 432]]  0.890722   \n",
      "13        LVQ  fold_3  0.876718  0.949672  [[299, 73], [23, 434]]  0.900415   \n",
      "14        LVQ  fold_4  0.903411  0.927790  [[327, 45], [33, 424]]  0.915767   \n",
      "15        LVQ  fold_5  0.886198  0.947598  [[306, 65], [24, 434]]  0.907001   \n",
      "16        LVQ  fold_6  0.889729  0.951965  [[307, 64], [22, 436]]  0.910230   \n",
      "17        LVQ  fold_7  0.899863  0.934498  [[321, 50], [30, 428]]  0.914530   \n",
      "18        LVQ  fold_8  0.906158  0.949782  [[320, 51], [23, 435]]  0.921610   \n",
      "19        LVQ  fold_9  0.907505  0.949782  [[321, 50], [23, 435]]  0.922587   \n",
      "20        DTR  fold_0  0.935490  0.965066  [[337, 35], [16, 442]]  0.945455   \n",
      "21        DTR  fold_1  0.933169  0.949672  [[341, 31], [23, 434]]  0.941432   \n",
      "22        DTR  fold_2  0.926449  0.949672  [[336, 36], [23, 434]]  0.936354   \n",
      "23        DTR  fold_3  0.932763  0.964989  [[335, 37], [16, 441]]  0.943316   \n",
      "24        DTR  fold_4  0.937796  0.956236  [[342, 30], [20, 437]]  0.945887   \n",
      "25        DTR  fold_5  0.926765  0.969432  [[328, 43], [14, 444]]  0.939683   \n",
      "26        DTR  fold_6  0.927789  0.960699  [[332, 39], [18, 440]]  0.939168   \n",
      "27        DTR  fold_7  0.927141  0.943231  [[338, 33], [26, 432]]  0.936078   \n",
      "28        DTR  fold_8  0.936319  0.945415  [[344, 27], [25, 433]]  0.943355   \n",
      "29        DTR  fold_9  0.934783  0.958515  [[338, 33], [19, 439]]  0.944086   \n",
      "30        SVM  fold_0  0.959431  0.967249  [[354, 18], [15, 443]]  0.964091   \n",
      "31        SVM  fold_1  0.951581  0.964989  [[349, 23], [16, 441]]  0.957655   \n",
      "32        SVM  fold_2  0.951925  0.973742  [[346, 26], [12, 445]]  0.959052   \n",
      "33        SVM  fold_3  0.943516  0.964989  [[343, 29], [16, 441]]  0.951456   \n",
      "34        SVM  fold_4  0.964021  0.973742  [[355, 17], [12, 445]]  0.968444   \n",
      "35        SVM  fold_5  0.944865  0.975983  [[339, 32], [11, 447]]  0.954109   \n",
      "36        SVM  fold_6  0.948328  0.969432  [[344, 27], [14, 444]]  0.955867   \n",
      "37        SVM  fold_7  0.947748  0.962882  [[346, 25], [17, 441]]  0.954545   \n",
      "38        SVM  fold_8  0.956670  0.967249  [[351, 20], [15, 443]]  0.961998   \n",
      "39        SVM  fold_9  0.954043  0.978166  [[345, 26], [10, 448]]  0.961373   \n",
      "\n",
      "    training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0        0.150595        0.258595    0.038554        0.991020       0.961446   \n",
      "1        0.216386        0.258142    0.043426        0.989815       0.956574   \n",
      "2        0.157795        0.208168    0.034982        0.989681       0.965018   \n",
      "3        0.150365        0.200379    0.042220        0.990753       0.957780   \n",
      "4        0.158504        0.225396    0.036188        0.989815       0.963812   \n",
      "5        0.145381        0.208846    0.044632        0.991423       0.955368   \n",
      "6        0.150019        0.207699    0.031363        0.990217       0.968637   \n",
      "7        0.158299        0.207437    0.030157        0.989011       0.969843   \n",
      "8        0.247245        0.224001    0.032569        0.990753       0.967431   \n",
      "9        0.133836        0.233562    0.032569        0.989413       0.967431   \n",
      "10      18.022866        0.103858    0.103614        0.901622       0.896386   \n",
      "11      18.671879        0.095556    0.092883        0.900563       0.907117   \n",
      "12      18.596905        0.074591    0.127865        0.891450       0.872135   \n",
      "13      31.233515        0.103505    0.115802        0.883543       0.884198   \n",
      "14      20.103187        0.303124    0.094089        0.904985       0.905911   \n",
      "15      65.548773        0.300754    0.107358        0.903109       0.892642   \n",
      "16      66.400597        0.329551    0.103739        0.894532       0.896261   \n",
      "17      67.791979        0.109562    0.096502        0.900965       0.903498   \n",
      "18      40.109460        0.294844    0.089264        0.895202       0.910736   \n",
      "19      62.336055        0.276352    0.088058        0.902975       0.911942   \n",
      "20       1.338725        0.046830    0.061446        0.937408       0.938554   \n",
      "21       1.257717        0.048533    0.065139        0.938354       0.934861   \n",
      "22       1.304446        0.046874    0.071170        0.937148       0.928830   \n",
      "23       1.259512        0.051344    0.063932        0.937818       0.936068   \n",
      "24       1.242452        0.047267    0.060314        0.935674       0.939686   \n",
      "25       1.305387        0.041892    0.068758        0.938622       0.931242   \n",
      "26       1.244323        0.047341    0.068758        0.938354       0.931242   \n",
      "27       1.238985        0.031255    0.071170        0.937282       0.928830   \n",
      "28       1.259360        0.036992    0.062726        0.933798       0.937274   \n",
      "29       1.287800        0.047441    0.062726        0.937416       0.937274   \n",
      "30      10.698344        0.260947    0.039759        0.967029       0.960241   \n",
      "31      10.701469        0.253890    0.047045        0.967167       0.952955   \n",
      "32      10.856422        0.283101    0.045838        0.958054       0.954162   \n",
      "33      10.711261        0.267445    0.054282        0.960332       0.945718   \n",
      "34      10.487811        0.267704    0.034982        0.965827       0.965018   \n",
      "35      10.501683        0.261909    0.051870        0.962477       0.948130   \n",
      "36      10.455854        0.252820    0.049457        0.964755       0.950543   \n",
      "37       8.224359        0.095636    0.050663        0.963147       0.949337   \n",
      "38       4.609559        0.119945    0.042220        0.957652       0.957780   \n",
      "39       4.316487        0.125359    0.043426        0.962611       0.956574   \n",
      "\n",
      "    precision  \n",
      "0    0.965066  \n",
      "1    0.954644  \n",
      "2    0.959227  \n",
      "3    0.958696  \n",
      "4    0.965142  \n",
      "5    0.952688  \n",
      "6    0.957627  \n",
      "7    0.969631  \n",
      "8    0.969499  \n",
      "9    0.965443  \n",
      "10   0.894068  \n",
      "11   0.900844  \n",
      "12   0.842105  \n",
      "13   0.856016  \n",
      "14   0.904051  \n",
      "15   0.869739  \n",
      "16   0.872000  \n",
      "17   0.895397  \n",
      "18   0.895062  \n",
      "19   0.896907  \n",
      "20   0.926625  \n",
      "21   0.933333  \n",
      "22   0.923404  \n",
      "23   0.922594  \n",
      "24   0.935760  \n",
      "25   0.911704  \n",
      "26   0.918580  \n",
      "27   0.929032  \n",
      "28   0.941304  \n",
      "29   0.930085  \n",
      "30   0.960954  \n",
      "31   0.950431  \n",
      "32   0.944798  \n",
      "33   0.938298  \n",
      "34   0.963203  \n",
      "35   0.933194  \n",
      "36   0.942675  \n",
      "37   0.946352  \n",
      "38   0.956803  \n",
      "39   0.945148  \n",
      "Model RF\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "   model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0         KNN  fold_0  0.961027  0.965066  [[356, 16], [16, 442]]  0.965066   \n",
      "1         KNN  fold_1  0.955363  0.967177  [[351, 21], [15, 442]]  0.960870   \n",
      "2         KNN  fold_2  0.963521  0.978118  [[353, 19], [10, 447]]  0.968581   \n",
      "3         KNN  fold_3  0.956957  0.964989  [[353, 19], [16, 441]]  0.961832   \n",
      "4         KNN  fold_4  0.963177  0.969365  [[356, 16], [14, 443]]  0.967249   \n",
      "5         KNN  fold_5  0.953975  0.967249  [[349, 22], [15, 443]]  0.959913   \n",
      "6         KNN  fold_6  0.966496  0.986900   [[351, 20], [6, 452]]  0.972043   \n",
      "7         KNN  fold_7  0.969123  0.975983  [[357, 14], [11, 447]]  0.972797   \n",
      "8         KNN  fold_8  0.966940  0.971616  [[357, 14], [13, 445]]  0.970556   \n",
      "9         KNN  fold_9  0.966428  0.975983  [[355, 16], [11, 447]]  0.970684   \n",
      "10        LVQ  fold_0  0.893494  0.921397  [[322, 50], [36, 422]]  0.907527   \n",
      "11        LVQ  fold_1  0.904005  0.934354  [[325, 47], [30, 427]]  0.917293   \n",
      "12        LVQ  fold_2  0.863777  0.945295  [[291, 81], [25, 432]]  0.890722   \n",
      "13        LVQ  fold_3  0.876718  0.949672  [[299, 73], [23, 434]]  0.900415   \n",
      "14        LVQ  fold_4  0.903411  0.927790  [[327, 45], [33, 424]]  0.915767   \n",
      "15        LVQ  fold_5  0.886198  0.947598  [[306, 65], [24, 434]]  0.907001   \n",
      "16        LVQ  fold_6  0.889729  0.951965  [[307, 64], [22, 436]]  0.910230   \n",
      "17        LVQ  fold_7  0.899863  0.934498  [[321, 50], [30, 428]]  0.914530   \n",
      "18        LVQ  fold_8  0.906158  0.949782  [[320, 51], [23, 435]]  0.921610   \n",
      "19        LVQ  fold_9  0.907505  0.949782  [[321, 50], [23, 435]]  0.922587   \n",
      "20        DTR  fold_0  0.935490  0.965066  [[337, 35], [16, 442]]  0.945455   \n",
      "21        DTR  fold_1  0.933169  0.949672  [[341, 31], [23, 434]]  0.941432   \n",
      "22        DTR  fold_2  0.926449  0.949672  [[336, 36], [23, 434]]  0.936354   \n",
      "23        DTR  fold_3  0.932763  0.964989  [[335, 37], [16, 441]]  0.943316   \n",
      "24        DTR  fold_4  0.937796  0.956236  [[342, 30], [20, 437]]  0.945887   \n",
      "25        DTR  fold_5  0.926765  0.969432  [[328, 43], [14, 444]]  0.939683   \n",
      "26        DTR  fold_6  0.927789  0.960699  [[332, 39], [18, 440]]  0.939168   \n",
      "27        DTR  fold_7  0.927141  0.943231  [[338, 33], [26, 432]]  0.936078   \n",
      "28        DTR  fold_8  0.936319  0.945415  [[344, 27], [25, 433]]  0.943355   \n",
      "29        DTR  fold_9  0.934783  0.958515  [[338, 33], [19, 439]]  0.944086   \n",
      "30        SVM  fold_0  0.959431  0.967249  [[354, 18], [15, 443]]  0.964091   \n",
      "31        SVM  fold_1  0.951581  0.964989  [[349, 23], [16, 441]]  0.957655   \n",
      "32        SVM  fold_2  0.951925  0.973742  [[346, 26], [12, 445]]  0.959052   \n",
      "33        SVM  fold_3  0.943516  0.964989  [[343, 29], [16, 441]]  0.951456   \n",
      "34        SVM  fold_4  0.964021  0.973742  [[355, 17], [12, 445]]  0.968444   \n",
      "35        SVM  fold_5  0.944865  0.975983  [[339, 32], [11, 447]]  0.954109   \n",
      "36        SVM  fold_6  0.948328  0.969432  [[344, 27], [14, 444]]  0.955867   \n",
      "37        SVM  fold_7  0.947748  0.962882  [[346, 25], [17, 441]]  0.954545   \n",
      "38        SVM  fold_8  0.956670  0.967249  [[351, 20], [15, 443]]  0.961998   \n",
      "39        SVM  fold_9  0.954043  0.978166  [[345, 26], [10, 448]]  0.961373   \n",
      "40         RF  fold_0  0.962454  0.975983  [[353, 19], [11, 447]]  0.967532   \n",
      "41         RF  fold_1  0.963177  0.969365  [[356, 16], [14, 443]]  0.967249   \n",
      "42         RF  fold_2  0.969492  0.984683   [[355, 17], [7, 450]]  0.974026   \n",
      "43         RF  fold_3  0.957301  0.973742  [[350, 22], [12, 445]]  0.963203   \n",
      "44         RF  fold_4  0.963771  0.975930  [[354, 18], [11, 446]]  0.968512   \n",
      "45         RF  fold_5  0.960525  0.980349   [[349, 22], [9, 449]]  0.966631   \n",
      "46         RF  fold_6  0.961873  0.980349   [[350, 21], [9, 449]]  0.967672   \n",
      "47         RF  fold_7  0.963665  0.965066  [[357, 14], [16, 442]]  0.967177   \n",
      "48         RF  fold_8  0.971051  0.982533   [[356, 15], [8, 450]]  0.975081   \n",
      "49         RF  fold_9  0.960525  0.980349   [[349, 22], [9, 449]]  0.966631   \n",
      "\n",
      "    training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0        0.150595        0.258595    0.038554        0.991020       0.961446   \n",
      "1        0.216386        0.258142    0.043426        0.989815       0.956574   \n",
      "2        0.157795        0.208168    0.034982        0.989681       0.965018   \n",
      "3        0.150365        0.200379    0.042220        0.990753       0.957780   \n",
      "4        0.158504        0.225396    0.036188        0.989815       0.963812   \n",
      "5        0.145381        0.208846    0.044632        0.991423       0.955368   \n",
      "6        0.150019        0.207699    0.031363        0.990217       0.968637   \n",
      "7        0.158299        0.207437    0.030157        0.989011       0.969843   \n",
      "8        0.247245        0.224001    0.032569        0.990753       0.967431   \n",
      "9        0.133836        0.233562    0.032569        0.989413       0.967431   \n",
      "10      18.022866        0.103858    0.103614        0.901622       0.896386   \n",
      "11      18.671879        0.095556    0.092883        0.900563       0.907117   \n",
      "12      18.596905        0.074591    0.127865        0.891450       0.872135   \n",
      "13      31.233515        0.103505    0.115802        0.883543       0.884198   \n",
      "14      20.103187        0.303124    0.094089        0.904985       0.905911   \n",
      "15      65.548773        0.300754    0.107358        0.903109       0.892642   \n",
      "16      66.400597        0.329551    0.103739        0.894532       0.896261   \n",
      "17      67.791979        0.109562    0.096502        0.900965       0.903498   \n",
      "18      40.109460        0.294844    0.089264        0.895202       0.910736   \n",
      "19      62.336055        0.276352    0.088058        0.902975       0.911942   \n",
      "20       1.338725        0.046830    0.061446        0.937408       0.938554   \n",
      "21       1.257717        0.048533    0.065139        0.938354       0.934861   \n",
      "22       1.304446        0.046874    0.071170        0.937148       0.928830   \n",
      "23       1.259512        0.051344    0.063932        0.937818       0.936068   \n",
      "24       1.242452        0.047267    0.060314        0.935674       0.939686   \n",
      "25       1.305387        0.041892    0.068758        0.938622       0.931242   \n",
      "26       1.244323        0.047341    0.068758        0.938354       0.931242   \n",
      "27       1.238985        0.031255    0.071170        0.937282       0.928830   \n",
      "28       1.259360        0.036992    0.062726        0.933798       0.937274   \n",
      "29       1.287800        0.047441    0.062726        0.937416       0.937274   \n",
      "30      10.698344        0.260947    0.039759        0.967029       0.960241   \n",
      "31      10.701469        0.253890    0.047045        0.967167       0.952955   \n",
      "32      10.856422        0.283101    0.045838        0.958054       0.954162   \n",
      "33      10.711261        0.267445    0.054282        0.960332       0.945718   \n",
      "34      10.487811        0.267704    0.034982        0.965827       0.965018   \n",
      "35      10.501683        0.261909    0.051870        0.962477       0.948130   \n",
      "36      10.455854        0.252820    0.049457        0.964755       0.950543   \n",
      "37       8.224359        0.095636    0.050663        0.963147       0.949337   \n",
      "38       4.609559        0.119945    0.042220        0.957652       0.957780   \n",
      "39       4.316487        0.125359    0.043426        0.962611       0.956574   \n",
      "40       3.015382        0.073953    0.036145        0.980164       0.963855   \n",
      "41       2.913314        0.064450    0.036188        0.979764       0.963812   \n",
      "42       2.998008        0.066678    0.028951        0.978826       0.971049   \n",
      "43       3.087422        0.062896    0.041013        0.979898       0.958987   \n",
      "44       2.837864        0.081818    0.034982        0.980166       0.965018   \n",
      "45       2.816962        0.080482    0.037394        0.979496       0.962606   \n",
      "46       2.799946        0.079784    0.036188        0.979630       0.963812   \n",
      "47       2.892308        0.066223    0.036188        0.978826       0.963812   \n",
      "48       3.007813        0.060710    0.027744        0.979228       0.972256   \n",
      "49       2.849108        0.081285    0.037394        0.980032       0.962606   \n",
      "\n",
      "    precision  \n",
      "0    0.965066  \n",
      "1    0.954644  \n",
      "2    0.959227  \n",
      "3    0.958696  \n",
      "4    0.965142  \n",
      "5    0.952688  \n",
      "6    0.957627  \n",
      "7    0.969631  \n",
      "8    0.969499  \n",
      "9    0.965443  \n",
      "10   0.894068  \n",
      "11   0.900844  \n",
      "12   0.842105  \n",
      "13   0.856016  \n",
      "14   0.904051  \n",
      "15   0.869739  \n",
      "16   0.872000  \n",
      "17   0.895397  \n",
      "18   0.895062  \n",
      "19   0.896907  \n",
      "20   0.926625  \n",
      "21   0.933333  \n",
      "22   0.923404  \n",
      "23   0.922594  \n",
      "24   0.935760  \n",
      "25   0.911704  \n",
      "26   0.918580  \n",
      "27   0.929032  \n",
      "28   0.941304  \n",
      "29   0.930085  \n",
      "30   0.960954  \n",
      "31   0.950431  \n",
      "32   0.944798  \n",
      "33   0.938298  \n",
      "34   0.963203  \n",
      "35   0.933194  \n",
      "36   0.942675  \n",
      "37   0.946352  \n",
      "38   0.956803  \n",
      "39   0.945148  \n",
      "40   0.959227  \n",
      "41   0.965142  \n",
      "42   0.963597  \n",
      "43   0.952891  \n",
      "44   0.961207  \n",
      "45   0.953291  \n",
      "46   0.955319  \n",
      "47   0.969298  \n",
      "48   0.967742  \n",
      "49   0.953291  \n",
      "Model XGB\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "   model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0         KNN  fold_0  0.961027  0.965066  [[356, 16], [16, 442]]  0.965066   \n",
      "1         KNN  fold_1  0.955363  0.967177  [[351, 21], [15, 442]]  0.960870   \n",
      "2         KNN  fold_2  0.963521  0.978118  [[353, 19], [10, 447]]  0.968581   \n",
      "3         KNN  fold_3  0.956957  0.964989  [[353, 19], [16, 441]]  0.961832   \n",
      "4         KNN  fold_4  0.963177  0.969365  [[356, 16], [14, 443]]  0.967249   \n",
      "5         KNN  fold_5  0.953975  0.967249  [[349, 22], [15, 443]]  0.959913   \n",
      "6         KNN  fold_6  0.966496  0.986900   [[351, 20], [6, 452]]  0.972043   \n",
      "7         KNN  fold_7  0.969123  0.975983  [[357, 14], [11, 447]]  0.972797   \n",
      "8         KNN  fold_8  0.966940  0.971616  [[357, 14], [13, 445]]  0.970556   \n",
      "9         KNN  fold_9  0.966428  0.975983  [[355, 16], [11, 447]]  0.970684   \n",
      "10        LVQ  fold_0  0.893494  0.921397  [[322, 50], [36, 422]]  0.907527   \n",
      "11        LVQ  fold_1  0.904005  0.934354  [[325, 47], [30, 427]]  0.917293   \n",
      "12        LVQ  fold_2  0.863777  0.945295  [[291, 81], [25, 432]]  0.890722   \n",
      "13        LVQ  fold_3  0.876718  0.949672  [[299, 73], [23, 434]]  0.900415   \n",
      "14        LVQ  fold_4  0.903411  0.927790  [[327, 45], [33, 424]]  0.915767   \n",
      "15        LVQ  fold_5  0.886198  0.947598  [[306, 65], [24, 434]]  0.907001   \n",
      "16        LVQ  fold_6  0.889729  0.951965  [[307, 64], [22, 436]]  0.910230   \n",
      "17        LVQ  fold_7  0.899863  0.934498  [[321, 50], [30, 428]]  0.914530   \n",
      "18        LVQ  fold_8  0.906158  0.949782  [[320, 51], [23, 435]]  0.921610   \n",
      "19        LVQ  fold_9  0.907505  0.949782  [[321, 50], [23, 435]]  0.922587   \n",
      "20        DTR  fold_0  0.935490  0.965066  [[337, 35], [16, 442]]  0.945455   \n",
      "21        DTR  fold_1  0.933169  0.949672  [[341, 31], [23, 434]]  0.941432   \n",
      "22        DTR  fold_2  0.926449  0.949672  [[336, 36], [23, 434]]  0.936354   \n",
      "23        DTR  fold_3  0.932763  0.964989  [[335, 37], [16, 441]]  0.943316   \n",
      "24        DTR  fold_4  0.937796  0.956236  [[342, 30], [20, 437]]  0.945887   \n",
      "25        DTR  fold_5  0.926765  0.969432  [[328, 43], [14, 444]]  0.939683   \n",
      "26        DTR  fold_6  0.927789  0.960699  [[332, 39], [18, 440]]  0.939168   \n",
      "27        DTR  fold_7  0.927141  0.943231  [[338, 33], [26, 432]]  0.936078   \n",
      "28        DTR  fold_8  0.936319  0.945415  [[344, 27], [25, 433]]  0.943355   \n",
      "29        DTR  fold_9  0.934783  0.958515  [[338, 33], [19, 439]]  0.944086   \n",
      "30        SVM  fold_0  0.959431  0.967249  [[354, 18], [15, 443]]  0.964091   \n",
      "31        SVM  fold_1  0.951581  0.964989  [[349, 23], [16, 441]]  0.957655   \n",
      "32        SVM  fold_2  0.951925  0.973742  [[346, 26], [12, 445]]  0.959052   \n",
      "33        SVM  fold_3  0.943516  0.964989  [[343, 29], [16, 441]]  0.951456   \n",
      "34        SVM  fold_4  0.964021  0.973742  [[355, 17], [12, 445]]  0.968444   \n",
      "35        SVM  fold_5  0.944865  0.975983  [[339, 32], [11, 447]]  0.954109   \n",
      "36        SVM  fold_6  0.948328  0.969432  [[344, 27], [14, 444]]  0.955867   \n",
      "37        SVM  fold_7  0.947748  0.962882  [[346, 25], [17, 441]]  0.954545   \n",
      "38        SVM  fold_8  0.956670  0.967249  [[351, 20], [15, 443]]  0.961998   \n",
      "39        SVM  fold_9  0.954043  0.978166  [[345, 26], [10, 448]]  0.961373   \n",
      "40         RF  fold_0  0.962454  0.975983  [[353, 19], [11, 447]]  0.967532   \n",
      "41         RF  fold_1  0.963177  0.969365  [[356, 16], [14, 443]]  0.967249   \n",
      "42         RF  fold_2  0.969492  0.984683   [[355, 17], [7, 450]]  0.974026   \n",
      "43         RF  fold_3  0.957301  0.973742  [[350, 22], [12, 445]]  0.963203   \n",
      "44         RF  fold_4  0.963771  0.975930  [[354, 18], [11, 446]]  0.968512   \n",
      "45         RF  fold_5  0.960525  0.980349   [[349, 22], [9, 449]]  0.966631   \n",
      "46         RF  fold_6  0.961873  0.980349   [[350, 21], [9, 449]]  0.967672   \n",
      "47         RF  fold_7  0.963665  0.965066  [[357, 14], [16, 442]]  0.967177   \n",
      "48         RF  fold_8  0.971051  0.982533   [[356, 15], [8, 450]]  0.975081   \n",
      "49         RF  fold_9  0.960525  0.980349   [[349, 22], [9, 449]]  0.966631   \n",
      "50        XGB  fold_0  0.970518  0.975983  [[359, 13], [11, 447]]  0.973856   \n",
      "51        XGB  fold_1  0.968054  0.973742  [[358, 14], [12, 445]]  0.971616   \n",
      "52        XGB  fold_2  0.973524  0.984683   [[358, 14], [7, 450]]  0.977199   \n",
      "53        XGB  fold_3  0.959645  0.964989  [[355, 17], [16, 441]]  0.963934   \n",
      "54        XGB  fold_4  0.972930  0.978118  [[360, 12], [10, 447]]  0.975983   \n",
      "55        XGB  fold_5  0.965404  0.984716   [[351, 20], [7, 451]]  0.970936   \n",
      "56        XGB  fold_6  0.971374  0.991266   [[353, 18], [4, 454]]  0.976344   \n",
      "57        XGB  fold_7  0.970983  0.971616  [[360, 11], [13, 445]]  0.973742   \n",
      "58        XGB  fold_8  0.974002  0.980349   [[359, 12], [9, 449]]  0.977149   \n",
      "59        XGB  fold_9  0.969447  0.984716   [[354, 17], [7, 451]]  0.974082   \n",
      "\n",
      "    training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0        0.150595        0.258595    0.038554        0.991020       0.961446   \n",
      "1        0.216386        0.258142    0.043426        0.989815       0.956574   \n",
      "2        0.157795        0.208168    0.034982        0.989681       0.965018   \n",
      "3        0.150365        0.200379    0.042220        0.990753       0.957780   \n",
      "4        0.158504        0.225396    0.036188        0.989815       0.963812   \n",
      "5        0.145381        0.208846    0.044632        0.991423       0.955368   \n",
      "6        0.150019        0.207699    0.031363        0.990217       0.968637   \n",
      "7        0.158299        0.207437    0.030157        0.989011       0.969843   \n",
      "8        0.247245        0.224001    0.032569        0.990753       0.967431   \n",
      "9        0.133836        0.233562    0.032569        0.989413       0.967431   \n",
      "10      18.022866        0.103858    0.103614        0.901622       0.896386   \n",
      "11      18.671879        0.095556    0.092883        0.900563       0.907117   \n",
      "12      18.596905        0.074591    0.127865        0.891450       0.872135   \n",
      "13      31.233515        0.103505    0.115802        0.883543       0.884198   \n",
      "14      20.103187        0.303124    0.094089        0.904985       0.905911   \n",
      "15      65.548773        0.300754    0.107358        0.903109       0.892642   \n",
      "16      66.400597        0.329551    0.103739        0.894532       0.896261   \n",
      "17      67.791979        0.109562    0.096502        0.900965       0.903498   \n",
      "18      40.109460        0.294844    0.089264        0.895202       0.910736   \n",
      "19      62.336055        0.276352    0.088058        0.902975       0.911942   \n",
      "20       1.338725        0.046830    0.061446        0.937408       0.938554   \n",
      "21       1.257717        0.048533    0.065139        0.938354       0.934861   \n",
      "22       1.304446        0.046874    0.071170        0.937148       0.928830   \n",
      "23       1.259512        0.051344    0.063932        0.937818       0.936068   \n",
      "24       1.242452        0.047267    0.060314        0.935674       0.939686   \n",
      "25       1.305387        0.041892    0.068758        0.938622       0.931242   \n",
      "26       1.244323        0.047341    0.068758        0.938354       0.931242   \n",
      "27       1.238985        0.031255    0.071170        0.937282       0.928830   \n",
      "28       1.259360        0.036992    0.062726        0.933798       0.937274   \n",
      "29       1.287800        0.047441    0.062726        0.937416       0.937274   \n",
      "30      10.698344        0.260947    0.039759        0.967029       0.960241   \n",
      "31      10.701469        0.253890    0.047045        0.967167       0.952955   \n",
      "32      10.856422        0.283101    0.045838        0.958054       0.954162   \n",
      "33      10.711261        0.267445    0.054282        0.960332       0.945718   \n",
      "34      10.487811        0.267704    0.034982        0.965827       0.965018   \n",
      "35      10.501683        0.261909    0.051870        0.962477       0.948130   \n",
      "36      10.455854        0.252820    0.049457        0.964755       0.950543   \n",
      "37       8.224359        0.095636    0.050663        0.963147       0.949337   \n",
      "38       4.609559        0.119945    0.042220        0.957652       0.957780   \n",
      "39       4.316487        0.125359    0.043426        0.962611       0.956574   \n",
      "40       3.015382        0.073953    0.036145        0.980164       0.963855   \n",
      "41       2.913314        0.064450    0.036188        0.979764       0.963812   \n",
      "42       2.998008        0.066678    0.028951        0.978826       0.971049   \n",
      "43       3.087422        0.062896    0.041013        0.979898       0.958987   \n",
      "44       2.837864        0.081818    0.034982        0.980166       0.965018   \n",
      "45       2.816962        0.080482    0.037394        0.979496       0.962606   \n",
      "46       2.799946        0.079784    0.036188        0.979630       0.963812   \n",
      "47       2.892308        0.066223    0.036188        0.978826       0.963812   \n",
      "48       3.007813        0.060710    0.027744        0.979228       0.972256   \n",
      "49       2.849108        0.081285    0.037394        0.980032       0.962606   \n",
      "50       3.327116        0.010613    0.028916        0.989546       0.971084   \n",
      "51       3.408116        0.014066    0.031363        0.989413       0.968637   \n",
      "52       3.245558        0.000000    0.025332        0.989145       0.974668   \n",
      "53       3.108377        0.013917    0.039807        0.989681       0.960193   \n",
      "54       3.134388        0.000000    0.026538        0.988207       0.973462   \n",
      "55       3.433359        0.016898    0.032569        0.989145       0.967431   \n",
      "56       3.474054        0.009282    0.026538        0.990083       0.973462   \n",
      "57       3.644018        0.007785    0.028951        0.988609       0.971049   \n",
      "58       3.379151        0.006769    0.025332        0.988877       0.974668   \n",
      "59       3.366447        0.008769    0.028951        0.989413       0.971049   \n",
      "\n",
      "    precision  \n",
      "0    0.965066  \n",
      "1    0.954644  \n",
      "2    0.959227  \n",
      "3    0.958696  \n",
      "4    0.965142  \n",
      "5    0.952688  \n",
      "6    0.957627  \n",
      "7    0.969631  \n",
      "8    0.969499  \n",
      "9    0.965443  \n",
      "10   0.894068  \n",
      "11   0.900844  \n",
      "12   0.842105  \n",
      "13   0.856016  \n",
      "14   0.904051  \n",
      "15   0.869739  \n",
      "16   0.872000  \n",
      "17   0.895397  \n",
      "18   0.895062  \n",
      "19   0.896907  \n",
      "20   0.926625  \n",
      "21   0.933333  \n",
      "22   0.923404  \n",
      "23   0.922594  \n",
      "24   0.935760  \n",
      "25   0.911704  \n",
      "26   0.918580  \n",
      "27   0.929032  \n",
      "28   0.941304  \n",
      "29   0.930085  \n",
      "30   0.960954  \n",
      "31   0.950431  \n",
      "32   0.944798  \n",
      "33   0.938298  \n",
      "34   0.963203  \n",
      "35   0.933194  \n",
      "36   0.942675  \n",
      "37   0.946352  \n",
      "38   0.956803  \n",
      "39   0.945148  \n",
      "40   0.959227  \n",
      "41   0.965142  \n",
      "42   0.963597  \n",
      "43   0.952891  \n",
      "44   0.961207  \n",
      "45   0.953291  \n",
      "46   0.955319  \n",
      "47   0.969298  \n",
      "48   0.967742  \n",
      "49   0.953291  \n",
      "50   0.971739  \n",
      "51   0.969499  \n",
      "52   0.969828  \n",
      "53   0.962882  \n",
      "54   0.973856  \n",
      "55   0.957537  \n",
      "56   0.961864  \n",
      "57   0.975877  \n",
      "58   0.973970  \n",
      "59   0.963675  \n",
      "Model LGBM\n",
      "Fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:150: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] O sistema não pode encontrar o arquivo especificado\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 227, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4118, number of negative: 3343\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002400 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 7461, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551937 -> initscore=0.208499\n",
      "[LightGBM] [Info] Start training from score 0.208499\n",
      "Fold: 1\n",
      "[LightGBM] [Info] Number of positive: 4119, number of negative: 3343\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001103 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 7462, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551997 -> initscore=0.208742\n",
      "[LightGBM] [Info] Start training from score 0.208742\n",
      "Fold: 2\n",
      "[LightGBM] [Info] Number of positive: 4119, number of negative: 3343\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 7462, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551997 -> initscore=0.208742\n",
      "[LightGBM] [Info] Start training from score 0.208742\n",
      "Fold: 3\n",
      "[LightGBM] [Info] Number of positive: 4119, number of negative: 3343\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001215 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 7462, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551997 -> initscore=0.208742\n",
      "[LightGBM] [Info] Start training from score 0.208742\n",
      "Fold: 4\n",
      "[LightGBM] [Info] Number of positive: 4119, number of negative: 3343\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001297 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 7462, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551997 -> initscore=0.208742\n",
      "[LightGBM] [Info] Start training from score 0.208742\n",
      "Fold: 5\n",
      "[LightGBM] [Info] Number of positive: 4118, number of negative: 3344\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 7462, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551863 -> initscore=0.208200\n",
      "[LightGBM] [Info] Start training from score 0.208200\n",
      "Fold: 6\n",
      "[LightGBM] [Info] Number of positive: 4118, number of negative: 3344\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001543 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 7462, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551863 -> initscore=0.208200\n",
      "[LightGBM] [Info] Start training from score 0.208200\n",
      "Fold: 7\n",
      "[LightGBM] [Info] Number of positive: 4118, number of negative: 3344\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001380 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 7462, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551863 -> initscore=0.208200\n",
      "[LightGBM] [Info] Start training from score 0.208200\n",
      "Fold: 8\n",
      "[LightGBM] [Info] Number of positive: 4118, number of negative: 3344\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001496 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 7462, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551863 -> initscore=0.208200\n",
      "[LightGBM] [Info] Start training from score 0.208200\n",
      "Fold: 9\n",
      "[LightGBM] [Info] Number of positive: 4118, number of negative: 3344\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 7462, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551863 -> initscore=0.208200\n",
      "[LightGBM] [Info] Start training from score 0.208200\n",
      "\n",
      " Metrics: \n",
      "   model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0         KNN  fold_0  0.961027  0.965066  [[356, 16], [16, 442]]  0.965066   \n",
      "1         KNN  fold_1  0.955363  0.967177  [[351, 21], [15, 442]]  0.960870   \n",
      "2         KNN  fold_2  0.963521  0.978118  [[353, 19], [10, 447]]  0.968581   \n",
      "3         KNN  fold_3  0.956957  0.964989  [[353, 19], [16, 441]]  0.961832   \n",
      "4         KNN  fold_4  0.963177  0.969365  [[356, 16], [14, 443]]  0.967249   \n",
      "..        ...     ...       ...       ...                     ...       ...   \n",
      "65       LGBM  fold_5  0.970539  0.986900   [[354, 17], [6, 452]]  0.975189   \n",
      "66       LGBM  fold_6  0.974326  0.989083   [[356, 15], [5, 453]]  0.978402   \n",
      "67       LGBM  fold_7  0.971819  0.975983  [[359, 12], [11, 447]]  0.974918   \n",
      "68       LGBM  fold_8  0.973166  0.975983  [[360, 11], [11, 447]]  0.975983   \n",
      "69       LGBM  fold_9  0.967264  0.980349   [[354, 17], [9, 449]]  0.971861   \n",
      "\n",
      "    training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0        0.150595        0.258595    0.038554        0.991020       0.961446   \n",
      "1        0.216386        0.258142    0.043426        0.989815       0.956574   \n",
      "2        0.157795        0.208168    0.034982        0.989681       0.965018   \n",
      "3        0.150365        0.200379    0.042220        0.990753       0.957780   \n",
      "4        0.158504        0.225396    0.036188        0.989815       0.963812   \n",
      "..            ...             ...         ...             ...            ...   \n",
      "65       1.617600        0.003036    0.027744        0.990485       0.972256   \n",
      "66       1.725353        0.000000    0.024125        0.990753       0.975875   \n",
      "67       1.572685        0.000000    0.027744        0.989547       0.972256   \n",
      "68       1.718928        0.003969    0.026538        0.989547       0.973462   \n",
      "69       1.664020        0.009906    0.031363        0.990217       0.968637   \n",
      "\n",
      "    precision  \n",
      "0    0.965066  \n",
      "1    0.954644  \n",
      "2    0.959227  \n",
      "3    0.958696  \n",
      "4    0.965142  \n",
      "..        ...  \n",
      "65   0.963753  \n",
      "66   0.967949  \n",
      "67   0.973856  \n",
      "68   0.975983  \n",
      "69   0.963519  \n",
      "\n",
      "[70 rows x 12 columns]\n",
      "Model MLP\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "   model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0         KNN  fold_0  0.961027  0.965066  [[356, 16], [16, 442]]  0.965066   \n",
      "1         KNN  fold_1  0.955363  0.967177  [[351, 21], [15, 442]]  0.960870   \n",
      "2         KNN  fold_2  0.963521  0.978118  [[353, 19], [10, 447]]  0.968581   \n",
      "3         KNN  fold_3  0.956957  0.964989  [[353, 19], [16, 441]]  0.961832   \n",
      "4         KNN  fold_4  0.963177  0.969365  [[356, 16], [14, 443]]  0.967249   \n",
      "..        ...     ...       ...       ...                     ...       ...   \n",
      "75        MLP  fold_5  0.958854  0.971616  [[351, 20], [13, 445]]  0.964247   \n",
      "76        MLP  fold_6  0.969123  0.975983  [[357, 14], [11, 447]]  0.972797   \n",
      "77        MLP  fold_7  0.972910  0.978166  [[359, 12], [10, 448]]  0.976035   \n",
      "78        MLP  fold_8  0.976954  0.978166   [[362, 9], [10, 448]]  0.979235   \n",
      "79        MLP  fold_9  0.962129  0.978166  [[351, 20], [10, 448]]  0.967603   \n",
      "\n",
      "    training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0        0.150595        0.258595    0.038554        0.991020       0.961446   \n",
      "1        0.216386        0.258142    0.043426        0.989815       0.956574   \n",
      "2        0.157795        0.208168    0.034982        0.989681       0.965018   \n",
      "3        0.150365        0.200379    0.042220        0.990753       0.957780   \n",
      "4        0.158504        0.225396    0.036188        0.989815       0.963812   \n",
      "..            ...             ...         ...             ...            ...   \n",
      "75      38.277667        0.000000    0.039807        0.990753       0.960193   \n",
      "76      33.282256        0.004445    0.030157        0.991021       0.969843   \n",
      "77      31.769971        0.016021    0.026538        0.989681       0.973462   \n",
      "78      39.856768        0.008228    0.022919        0.989949       0.977081   \n",
      "79      31.650758        0.007514    0.036188        0.989815       0.963812   \n",
      "\n",
      "    precision  \n",
      "0    0.965066  \n",
      "1    0.954644  \n",
      "2    0.959227  \n",
      "3    0.958696  \n",
      "4    0.965142  \n",
      "..        ...  \n",
      "75   0.956989  \n",
      "76   0.969631  \n",
      "77   0.973913  \n",
      "78   0.980306  \n",
      "79   0.957265  \n",
      "\n",
      "[80 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "cv = cross_validation(X_train, y_train, best_pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Heterogêneo\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "    model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0  Heterogêneo  fold_0  0.974885  0.984716   [[359, 13], [7, 451]]  0.978308   \n",
      "1  Heterogêneo  fold_1  0.970242  0.978118  [[358, 14], [10, 447]]  0.973856   \n",
      "2  Heterogêneo  fold_2  0.971680  0.989059   [[355, 17], [5, 452]]  0.976242   \n",
      "3  Heterogêneo  fold_3  0.962927  0.971554  [[355, 17], [13, 444]]  0.967320   \n",
      "4  Heterogêneo  fold_4  0.970742  0.973742  [[360, 12], [12, 445]]  0.973742   \n",
      "5  Heterogêneo  fold_5  0.966752  0.984716   [[352, 19], [7, 451]]  0.971983   \n",
      "6  Heterogêneo  fold_6  0.970539  0.986900   [[354, 17], [6, 452]]  0.975189   \n",
      "7  Heterogêneo  fold_7  0.982412  0.989083    [[362, 9], [5, 453]]  0.984783   \n",
      "8  Heterogêneo  fold_8  0.974258  0.978166  [[360, 11], [10, 448]]  0.977099   \n",
      "9  Heterogêneo  fold_9  0.969447  0.984716   [[354, 17], [7, 451]]  0.974082   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0       7.060600        0.344222    0.024096        0.990752       0.975904   \n",
      "1       7.649313        0.342451    0.028951        0.989949       0.971049   \n",
      "2       6.496947        0.332809    0.026538        0.989949       0.973462   \n",
      "3       6.315637        0.271440    0.036188        0.990485       0.963812   \n",
      "4       7.024124        0.598225    0.028951        0.989815       0.971049   \n",
      "5      12.987120        0.582011    0.031363        0.991021       0.968637   \n",
      "6      13.307468        0.424559    0.027744        0.990485       0.972256   \n",
      "7       6.128471        0.296288    0.016888        0.989279       0.983112   \n",
      "8       6.333743        0.302859    0.025332        0.990083       0.974668   \n",
      "9       5.934883        0.264646    0.028951        0.990217       0.971049   \n",
      "\n",
      "   precision  \n",
      "0   0.971983  \n",
      "1   0.969631  \n",
      "2   0.963753  \n",
      "3   0.963124  \n",
      "4   0.973742  \n",
      "5   0.959574  \n",
      "6   0.963753  \n",
      "7   0.980519  \n",
      "8   0.976035  \n",
      "9   0.963675  \n",
      "Model ANNs\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "     model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0   Heterogêneo  fold_0  0.974885  0.984716   [[359, 13], [7, 451]]  0.978308   \n",
      "1   Heterogêneo  fold_1  0.970242  0.978118  [[358, 14], [10, 447]]  0.973856   \n",
      "2   Heterogêneo  fold_2  0.971680  0.989059   [[355, 17], [5, 452]]  0.976242   \n",
      "3   Heterogêneo  fold_3  0.962927  0.971554  [[355, 17], [13, 444]]  0.967320   \n",
      "4   Heterogêneo  fold_4  0.970742  0.973742  [[360, 12], [12, 445]]  0.973742   \n",
      "5   Heterogêneo  fold_5  0.966752  0.984716   [[352, 19], [7, 451]]  0.971983   \n",
      "6   Heterogêneo  fold_6  0.970539  0.986900   [[354, 17], [6, 452]]  0.975189   \n",
      "7   Heterogêneo  fold_7  0.982412  0.989083    [[362, 9], [5, 453]]  0.984783   \n",
      "8   Heterogêneo  fold_8  0.974258  0.978166  [[360, 11], [10, 448]]  0.977099   \n",
      "9   Heterogêneo  fold_9  0.969447  0.984716   [[354, 17], [7, 451]]  0.974082   \n",
      "10         ANNs  fold_0  0.966404  0.965066  [[360, 12], [16, 442]]  0.969298   \n",
      "11         ANNs  fold_1  0.968898  0.978118  [[357, 15], [10, 447]]  0.972797   \n",
      "12         ANNs  fold_2  0.974368  0.989059   [[357, 15], [5, 452]]  0.978355   \n",
      "13         ANNs  fold_3  0.965271  0.962801  [[360, 12], [17, 440]]  0.968097   \n",
      "14         ANNs  fold_4  0.963177  0.969365  [[356, 16], [14, 443]]  0.967249   \n",
      "15         ANNs  fold_5  0.962964  0.982533   [[350, 21], [8, 450]]  0.968784   \n",
      "16         ANNs  fold_6  0.967008  0.982533   [[353, 18], [8, 450]]  0.971922   \n",
      "17         ANNs  fold_7  0.979069  0.971616   [[366, 5], [13, 445]]  0.980176   \n",
      "18         ANNs  fold_8  0.972654  0.980349   [[358, 13], [9, 449]]  0.976087   \n",
      "19         ANNs  fold_9  0.973490  0.984716   [[357, 14], [7, 451]]  0.977248   \n",
      "\n",
      "    training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0        7.060600        0.344222    0.024096        0.990752       0.975904   \n",
      "1        7.649313        0.342451    0.028951        0.989949       0.971049   \n",
      "2        6.496947        0.332809    0.026538        0.989949       0.973462   \n",
      "3        6.315637        0.271440    0.036188        0.990485       0.963812   \n",
      "4        7.024124        0.598225    0.028951        0.989815       0.971049   \n",
      "5       12.987120        0.582011    0.031363        0.991021       0.968637   \n",
      "6       13.307468        0.424559    0.027744        0.990485       0.972256   \n",
      "7        6.128471        0.296288    0.016888        0.989279       0.983112   \n",
      "8        6.333743        0.302859    0.025332        0.990083       0.974668   \n",
      "9        5.934883        0.264646    0.028951        0.990217       0.971049   \n",
      "10     104.134757        0.015638    0.033735        0.990752       0.966265   \n",
      "11     103.611469        0.021456    0.030157        0.990753       0.969843   \n",
      "12      81.039746        0.015717    0.024125        0.990485       0.975875   \n",
      "13      83.624569        0.015656    0.034982        0.990485       0.965018   \n",
      "14      88.610388        0.017072    0.036188        0.991021       0.963812   \n",
      "15      92.247695        0.016465    0.034982        0.991289       0.965018   \n",
      "16      72.633571        0.016526    0.031363        0.991021       0.968637   \n",
      "17      97.384375        0.015011    0.021713        0.989815       0.978287   \n",
      "18      89.748179        0.015559    0.026538        0.989815       0.973462   \n",
      "19      98.415097        0.016136    0.025332        0.990753       0.974668   \n",
      "\n",
      "    precision  \n",
      "0    0.971983  \n",
      "1    0.969631  \n",
      "2    0.963753  \n",
      "3    0.963124  \n",
      "4    0.973742  \n",
      "5    0.959574  \n",
      "6    0.963753  \n",
      "7    0.980519  \n",
      "8    0.976035  \n",
      "9    0.963675  \n",
      "10   0.973568  \n",
      "11   0.967532  \n",
      "12   0.967880  \n",
      "13   0.973451  \n",
      "14   0.965142  \n",
      "15   0.955414  \n",
      "16   0.961538  \n",
      "17   0.988889  \n",
      "18   0.971861  \n",
      "19   0.969892  \n"
     ]
    }
   ],
   "source": [
    "cv = cross_validation(X_train, y_train, ensembles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation with ADASYN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBS: NÃO FOI INCLUÍDO NO RELATÓRIO FINAL POIS OBSERVOU-SE QUE O ADASYN PIOROU O RECALL DOS MODELOS LGBM E XGBOOST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model KNN\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0        KNN  fold_0  0.968587  0.969432  [[360, 12], [14, 444]]  0.971554   \n",
      "1        KNN  fold_1  0.958957  0.947484  [[361, 11], [24, 433]]  0.961154   \n",
      "2        KNN  fold_2  0.954769  0.960613  [[353, 19], [18, 439]]  0.959563   \n",
      "3        KNN  fold_3  0.962239  0.954048  [[361, 11], [21, 436]]  0.964602   \n",
      "4        KNN  fold_4  0.959051  0.958425  [[357, 15], [19, 438]]  0.962637   \n",
      "5        KNN  fold_5  0.957694  0.958515  [[355, 16], [19, 439]]  0.961665   \n",
      "6        KNN  fold_6  0.966684  0.973799  [[356, 15], [12, 446]]  0.970620   \n",
      "7        KNN  fold_7  0.969891  0.969432  [[360, 11], [14, 444]]  0.972618   \n",
      "8        KNN  fold_8  0.964433  0.958515  [[360, 11], [19, 439]]  0.966960   \n",
      "9        KNN  fold_9  0.956347  0.958515  [[354, 17], [19, 439]]  0.960613   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0       0.250706        0.239468    0.031325        0.987926       0.968675   \n",
      "1       0.300168        0.208635    0.042220        0.987619       0.957780   \n",
      "2       0.245538        0.224612    0.044632        0.986539       0.955368   \n",
      "3       0.189498        0.205282    0.038601        0.988763       0.961399   \n",
      "4       0.221826        0.287457    0.041013        0.988417       0.958987   \n",
      "5       0.247940        0.227735    0.042220        0.988411       0.957780   \n",
      "6       0.181803        0.234867    0.032569        0.987979       0.967431   \n",
      "7       0.247396        0.220148    0.030157        0.986615       0.969843   \n",
      "8       0.223424        0.220414    0.036188        0.988101       0.963812   \n",
      "9       0.251178        0.238649    0.043426        0.987970       0.956574   \n",
      "\n",
      "   precision  \n",
      "0   0.973684  \n",
      "1   0.975225  \n",
      "2   0.958515  \n",
      "3   0.975391  \n",
      "4   0.966887  \n",
      "5   0.964835  \n",
      "6   0.967462  \n",
      "7   0.975824  \n",
      "8   0.975556  \n",
      "9   0.962719  \n",
      "Model LVQ\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                       CM  f1_score  \\\n",
      "0        LVQ  fold_0  0.819775  0.698690  [[350, 22], [138, 320]]  0.800000   \n",
      "1        LVQ  fold_1  0.827322  0.676149   [[364, 8], [148, 309]]  0.798450   \n",
      "2        LVQ  fold_2  0.851486  0.735230  [[360, 12], [121, 336]]  0.834783   \n",
      "3        LVQ  fold_3  0.846454  0.743982  [[353, 19], [117, 340]]  0.833333   \n",
      "4        LVQ  fold_4  0.866115  0.748359   [[366, 6], [115, 342]]  0.849689   \n",
      "5        LVQ  fold_5  0.782886  0.587336   [[363, 8], [189, 269]]  0.731973   \n",
      "6        LVQ  fold_6  0.835491  0.724891  [[351, 20], [126, 332]]  0.819753   \n",
      "7        LVQ  fold_7  0.859750  0.724891   [[369, 2], [126, 332]]  0.838384   \n",
      "8        LVQ  fold_8  0.862784  0.779476  [[351, 20], [101, 357]]  0.855090   \n",
      "9        LVQ  fold_9  0.835221  0.681223   [[367, 4], [146, 312]]  0.806202   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0      19.387875        0.064213    0.192771        0.834329       0.807229   \n",
      "1      24.384108        0.251323    0.188179        0.841528       0.811821   \n",
      "2      64.539009        0.267212    0.160434        0.832856       0.839566   \n",
      "3      65.632596        0.282582    0.164053        0.847547       0.835947   \n",
      "4      69.179153        0.283088    0.145959        0.846354       0.854041   \n",
      "5      67.186531        0.267359    0.237636        0.802862       0.762364   \n",
      "6      51.531852        0.086559    0.176116        0.836965       0.823884   \n",
      "7      39.007076        0.083432    0.154403        0.851013       0.845597   \n",
      "8      19.059355        0.096969    0.145959        0.850952       0.854041   \n",
      "9      21.283558        0.103943    0.180941        0.822385       0.819059   \n",
      "\n",
      "   precision  \n",
      "0   0.935673  \n",
      "1   0.974763  \n",
      "2   0.965517  \n",
      "3   0.947075  \n",
      "4   0.982759  \n",
      "5   0.971119  \n",
      "6   0.943182  \n",
      "7   0.994012  \n",
      "8   0.946950  \n",
      "9   0.987342  \n",
      "Model DTR\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0        DTR  fold_0  0.937761  0.945415  [[346, 26], [25, 433]]  0.944384   \n",
      "1        DTR  fold_1  0.924572  0.919037  [[346, 26], [37, 420]]  0.930233   \n",
      "2        DTR  fold_2  0.927105  0.932166  [[343, 29], [31, 426]]  0.934211   \n",
      "3        DTR  fold_3  0.931637  0.927790  [[348, 24], [33, 424]]  0.937017   \n",
      "4        DTR  fold_4  0.937857  0.932166  [[351, 21], [31, 426]]  0.942478   \n",
      "5        DTR  fold_5  0.930672  0.947598  [[339, 32], [24, 434]]  0.939394   \n",
      "6        DTR  fold_6  0.926885  0.945415  [[337, 34], [25, 433]]  0.936216   \n",
      "7        DTR  fold_7  0.933812  0.932314  [[347, 24], [31, 427]]  0.939494   \n",
      "8        DTR  fold_8  0.936319  0.945415  [[344, 27], [25, 433]]  0.943355   \n",
      "9        DTR  fold_9  0.933812  0.932314  [[347, 24], [31, 427]]  0.939494   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0       0.875352        0.016028    0.061446        0.925317       0.938554   \n",
      "1       0.833487        0.016817    0.075995        0.930669       0.924005   \n",
      "2       0.731913        0.008155    0.072376        0.929079       0.927624   \n",
      "3       0.662444        0.007699    0.068758        0.931577       0.931242   \n",
      "4       0.641401        0.008348    0.062726        0.927550       0.937274   \n",
      "5       0.754566        0.015594    0.067551        0.929872       0.932449   \n",
      "6       0.643331        0.015620    0.071170        0.931004       0.928830   \n",
      "7       0.669917        0.015626    0.066345        0.931323       0.933655   \n",
      "8       0.704059        0.017394    0.062726        0.928983       0.937274   \n",
      "9       0.693023        0.025252    0.066345        0.930298       0.933655   \n",
      "\n",
      "   precision  \n",
      "0   0.943355  \n",
      "1   0.941704  \n",
      "2   0.936264  \n",
      "3   0.946429  \n",
      "4   0.953020  \n",
      "5   0.931330  \n",
      "6   0.927195  \n",
      "7   0.946785  \n",
      "8   0.941304  \n",
      "9   0.946785  \n",
      "Model SVM\n",
      "Fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0        SVM  fold_0  0.954477  0.951965  [[356, 16], [22, 436]]  0.958242   \n",
      "1        SVM  fold_1  0.947610  0.940919  [[355, 17], [27, 430]]  0.951327   \n",
      "2        SVM  fold_2  0.951236  0.956236  [[352, 20], [20, 437]]  0.956236   \n",
      "3        SVM  fold_3  0.942734  0.936543  [[353, 19], [29, 428]]  0.946903   \n",
      "4        SVM  fold_4  0.951392  0.943107  [[357, 15], [26, 431]]  0.954596   \n",
      "5        SVM  fold_5  0.954743  0.960699  [[352, 19], [18, 440]]  0.959651   \n",
      "6        SVM  fold_6  0.953395  0.960699  [[351, 20], [18, 440]]  0.958606   \n",
      "7        SVM  fold_7  0.953839  0.945415  [[357, 14], [25, 433]]  0.956906   \n",
      "8        SVM  fold_8  0.956859  0.954148  [[356, 15], [21, 437]]  0.960440   \n",
      "9        SVM  fold_9  0.946589  0.949782  [[350, 21], [23, 435]]  0.951860   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0       5.478913        0.158226    0.045783        0.961663       0.954217   \n",
      "1       5.979058        0.156816    0.053076        0.952364       0.946924   \n",
      "2       5.410573        0.148789    0.048251        0.957497       0.951749   \n",
      "3       4.625450        0.141443    0.057901        0.956923       0.942099   \n",
      "4       5.054898        0.109897    0.049457        0.963243       0.950543   \n",
      "5       5.182269        0.117091    0.044632        0.955889       0.955368   \n",
      "6       4.557372        0.126407    0.045838        0.962059       0.954162   \n",
      "7       4.772424        0.125413    0.047045        0.961596       0.952955   \n",
      "8       4.786863        0.106415    0.043426        0.955912       0.956574   \n",
      "9       4.886899        0.124616    0.053076        0.965326       0.946924   \n",
      "\n",
      "   precision  \n",
      "0   0.964602  \n",
      "1   0.961969  \n",
      "2   0.956236  \n",
      "3   0.957494  \n",
      "4   0.966368  \n",
      "5   0.958606  \n",
      "6   0.956522  \n",
      "7   0.968680  \n",
      "8   0.966814  \n",
      "9   0.953947  \n",
      "Model RF\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0         RF  fold_0  0.970771  0.973799  [[360, 12], [12, 446]]  0.973799   \n",
      "1         RF  fold_1  0.960051  0.949672  [[361, 11], [23, 434]]  0.962306   \n",
      "2         RF  fold_2  0.962583  0.962801  [[358, 14], [17, 440]]  0.965971   \n",
      "3         RF  fold_3  0.957863  0.945295  [[361, 11], [25, 432]]  0.960000   \n",
      "4         RF  fold_4  0.962583  0.962801  [[358, 14], [17, 440]]  0.965971   \n",
      "5         RF  fold_5  0.965269  0.962882  [[359, 12], [17, 441]]  0.968167   \n",
      "6         RF  fold_6  0.963153  0.969432  [[355, 16], [14, 444]]  0.967320   \n",
      "7         RF  fold_7  0.966037  0.956332   [[362, 9], [20, 438]]  0.967956   \n",
      "8         RF  fold_8  0.968544  0.969432  [[359, 12], [14, 444]]  0.971554   \n",
      "9         RF  fold_9  0.959622  0.965066  [[354, 17], [16, 442]]  0.964013   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0       3.296474        0.080788    0.028916        0.977969       0.971084   \n",
      "1       3.617173        0.094233    0.041013        0.979248       0.958987   \n",
      "2       5.127136        0.173611    0.037394        0.978188       0.962606   \n",
      "3       6.165690        0.171973    0.043426        0.977775       0.956574   \n",
      "4       6.635106        0.189083    0.037394        0.979435       0.962606   \n",
      "5       6.525747        0.172647    0.034982        0.978122       0.965018   \n",
      "6       6.116442        0.173001    0.036188        0.979339       0.963812   \n",
      "7       6.270835        0.188696    0.034982        0.977233       0.965018   \n",
      "8       6.254840        0.173179    0.031363        0.977330       0.968637   \n",
      "9       5.841025        0.062390    0.039807        0.978771       0.960193   \n",
      "\n",
      "   precision  \n",
      "0   0.973799  \n",
      "1   0.975281  \n",
      "2   0.969163  \n",
      "3   0.975169  \n",
      "4   0.969163  \n",
      "5   0.973510  \n",
      "6   0.965217  \n",
      "7   0.979866  \n",
      "8   0.973684  \n",
      "9   0.962963  \n",
      "Model XGB\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0        XGB  fold_0  0.975390  0.980349   [[361, 11], [9, 449]]  0.978214   \n",
      "1        XGB  fold_1  0.958207  0.954048  [[358, 14], [21, 436]]  0.961411   \n",
      "2        XGB  fold_2  0.970992  0.971554  [[361, 11], [13, 444]]  0.973684   \n",
      "3        XGB  fold_3  0.965521  0.960613  [[361, 11], [18, 439]]  0.968026   \n",
      "4        XGB  fold_4  0.967460  0.967177  [[360, 12], [15, 442]]  0.970362   \n",
      "5        XGB  fold_5  0.974770  0.973799   [[362, 9], [12, 446]]  0.976999   \n",
      "6        XGB  fold_6  0.968611  0.980349   [[355, 16], [9, 449]]  0.972914   \n",
      "7        XGB  fold_7  0.985040  0.978166   [[368, 3], [10, 448]]  0.985699   \n",
      "8        XGB  fold_8  0.970147  0.967249  [[361, 10], [15, 443]]  0.972558   \n",
      "9        XGB  fold_9  0.963989  0.973799  [[354, 17], [12, 446]]  0.968512   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0       3.781348        0.015939    0.024096        0.986433       0.975904   \n",
      "1       3.715124        0.007416    0.042220        0.986322       0.957780   \n",
      "2       3.517552        0.013513    0.028951        0.986414       0.971049   \n",
      "3       3.258914        0.015599    0.034982        0.986765       0.965018   \n",
      "4       3.878364        0.004583    0.032569        0.987354       0.967431   \n",
      "5       3.958274        0.015718    0.025332        0.986400       0.974668   \n",
      "6       3.561375        0.010011    0.030157        0.987353       0.969843   \n",
      "7       3.244496        0.008265    0.015682        0.985239       0.984318   \n",
      "8       3.214212        0.008159    0.030157        0.986348       0.969843   \n",
      "9       3.890573        0.017021    0.034982        0.986555       0.965018   \n",
      "\n",
      "   precision  \n",
      "0   0.976087  \n",
      "1   0.968889  \n",
      "2   0.975824  \n",
      "3   0.975556  \n",
      "4   0.973568  \n",
      "5   0.980220  \n",
      "6   0.965591  \n",
      "7   0.993348  \n",
      "8   0.977925  \n",
      "9   0.963283  \n",
      "Model LGBM\n",
      "Fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:150: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] O sistema não pode encontrar o arquivo especificado\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 227, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\giull\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4118, number of negative: 3916\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 8034, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.512572 -> initscore=0.050297\n",
      "[LightGBM] [Info] Start training from score 0.050297\n",
      "Fold: 1\n",
      "[LightGBM] [Info] Number of positive: 4119, number of negative: 4362\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001372 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 8481, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485674 -> initscore=-0.057320\n",
      "[LightGBM] [Info] Start training from score -0.057320\n",
      "Fold: 2\n",
      "[LightGBM] [Info] Number of positive: 4119, number of negative: 3904\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001319 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 8023, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513399 -> initscore=0.053609\n",
      "[LightGBM] [Info] Start training from score 0.053609\n",
      "Fold: 3\n",
      "[LightGBM] [Info] Number of positive: 4119, number of negative: 3890\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001492 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 8009, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.514296 -> initscore=0.057201\n",
      "[LightGBM] [Info] Start training from score 0.057201\n",
      "Fold: 4\n",
      "[LightGBM] [Info] Number of positive: 4119, number of negative: 4342\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 8461, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486822 -> initscore=-0.052725\n",
      "[LightGBM] [Info] Start training from score -0.052725\n",
      "Fold: 5\n",
      "[LightGBM] [Info] Number of positive: 4118, number of negative: 4338\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 8456, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486991 -> initscore=-0.052046\n",
      "[LightGBM] [Info] Start training from score -0.052046\n",
      "Fold: 6\n",
      "[LightGBM] [Info] Number of positive: 4118, number of negative: 3868\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001991 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 7986, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.515652 -> initscore=0.062630\n",
      "[LightGBM] [Info] Start training from score 0.062630\n",
      "Fold: 7\n",
      "[LightGBM] [Info] Number of positive: 4118, number of negative: 3876\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001464 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 7994, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.515136 -> initscore=0.060564\n",
      "[LightGBM] [Info] Start training from score 0.060564\n",
      "Fold: 8\n",
      "[LightGBM] [Info] Number of positive: 4118, number of negative: 3866\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004197 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 7984, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.515782 -> initscore=0.063147\n",
      "[LightGBM] [Info] Start training from score 0.063147\n",
      "Fold: 9\n",
      "[LightGBM] [Info] Number of positive: 4118, number of negative: 4361\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006662 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 89\n",
      "[LightGBM] [Info] Number of data points in the train set: 8479, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485670 -> initscore=-0.057334\n",
      "[LightGBM] [Info] Start training from score -0.057334\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0       LGBM  fold_0  0.965899  0.969432  [[358, 14], [14, 444]]  0.969432   \n",
      "1       LGBM  fold_1  0.958457  0.951860  [[359, 13], [22, 435]]  0.961326   \n",
      "2       LGBM  fold_2  0.968304  0.971554  [[359, 13], [13, 444]]  0.971554   \n",
      "3       LGBM  fold_3  0.966865  0.960613  [[362, 10], [18, 439]]  0.969095   \n",
      "4       LGBM  fold_4  0.970148  0.967177  [[362, 10], [15, 442]]  0.972497   \n",
      "5       LGBM  fold_5  0.970983  0.971616  [[360, 11], [13, 445]]  0.973742   \n",
      "6       LGBM  fold_6  0.969379  0.973799  [[358, 13], [12, 446]]  0.972737   \n",
      "7       LGBM  fold_7  0.976630  0.969432   [[365, 6], [14, 444]]  0.977974   \n",
      "8       LGBM  fold_8  0.976630  0.969432   [[365, 6], [14, 444]]  0.977974   \n",
      "9       LGBM  fold_9  0.966428  0.975983  [[355, 16], [11, 447]]  0.970684   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0       2.121940        0.000000    0.033735        0.987553       0.966265   \n",
      "1       1.783387        0.003509    0.042220        0.987148       0.957780   \n",
      "2       1.983034        0.000000    0.031363        0.987785       0.968637   \n",
      "3       2.148221        0.000000    0.033776        0.987889       0.966224   \n",
      "4       2.129578        0.008798    0.030157        0.988417       0.969843   \n",
      "5       2.417177        0.015625    0.028951        0.986518       0.971049   \n",
      "6       2.237861        0.008301    0.030157        0.987729       0.969843   \n",
      "7       4.033536        0.017561    0.024125        0.986365       0.975875   \n",
      "8       7.738467        0.015627    0.024125        0.987475       0.975875   \n",
      "9       8.740462        0.015626    0.032569        0.987263       0.967431   \n",
      "\n",
      "   precision  \n",
      "0   0.969432  \n",
      "1   0.970982  \n",
      "2   0.971554  \n",
      "3   0.977728  \n",
      "4   0.977876  \n",
      "5   0.975877  \n",
      "6   0.971678  \n",
      "7   0.986667  \n",
      "8   0.986667  \n",
      "9   0.965443  \n",
      "Model MLP\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0        MLP  fold_0  0.963129  0.958515  [[360, 12], [19, 439]]  0.965897   \n",
      "1        MLP  fold_1  0.955019  0.958425  [[354, 18], [19, 438]]  0.959474   \n",
      "2        MLP  fold_2  0.964771  0.967177  [[358, 14], [15, 442]]  0.968237   \n",
      "3        MLP  fold_3  0.959645  0.964989  [[355, 17], [16, 441]]  0.963934   \n",
      "4        MLP  fold_4  0.964677  0.956236  [[362, 10], [20, 437]]  0.966814   \n",
      "5        MLP  fold_5  0.960390  0.958515  [[357, 14], [19, 439]]  0.963776   \n",
      "6        MLP  fold_6  0.959878  0.962882  [[355, 16], [17, 441]]  0.963934   \n",
      "7        MLP  fold_7  0.970727  0.973799  [[359, 12], [12, 446]]  0.973799   \n",
      "8        MLP  fold_8  0.968220  0.960699   [[362, 9], [18, 440]]  0.970232   \n",
      "9        MLP  fold_9  0.957694  0.958515  [[355, 16], [19, 439]]  0.961665   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0      46.948097        0.000000    0.037349        0.987677       0.962651   \n",
      "1      32.311670        0.018141    0.044632        0.987384       0.955368   \n",
      "2      41.503662        0.000000    0.034982        0.987287       0.965018   \n",
      "3      35.030384        0.013088    0.039807        0.987264       0.960193   \n",
      "4      41.276962        0.000000    0.036188        0.988063       0.963812   \n",
      "5      38.689962        0.000000    0.039807        0.987465       0.960193   \n",
      "6      32.433755        0.003637    0.039807        0.987478       0.960193   \n",
      "7      33.728065        0.015574    0.028951        0.985864       0.971049   \n",
      "8      46.824081        0.007295    0.032569        0.987350       0.967431   \n",
      "9      25.413985        0.015555    0.042220        0.988206       0.957780   \n",
      "\n",
      "   precision  \n",
      "0   0.973392  \n",
      "1   0.960526  \n",
      "2   0.969298  \n",
      "3   0.962882  \n",
      "4   0.977629  \n",
      "5   0.969095  \n",
      "6   0.964989  \n",
      "7   0.973799  \n",
      "8   0.979955  \n",
      "9   0.964835  \n"
     ]
    }
   ],
   "source": [
    "cv = cross_validation(X_train, y_train, best_pipelines, use_adasyn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Heterogêneo\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "    model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0  Heterogêneo  fold_0  0.978330  0.978166   [[364, 8], [10, 448]]  0.980306   \n",
      "1  Heterogêneo  fold_1  0.963333  0.956236  [[361, 11], [20, 437]]  0.965746   \n",
      "2  Heterogêneo  fold_2  0.969648  0.971554  [[360, 12], [13, 444]]  0.972618   \n",
      "3  Heterogêneo  fold_3  0.967115  0.958425   [[363, 9], [19, 438]]  0.969027   \n",
      "4  Heterogêneo  fold_4  0.968804  0.967177  [[361, 11], [15, 442]]  0.971429   \n",
      "5  Heterogêneo  fold_5  0.967452  0.967249  [[359, 12], [15, 443]]  0.970427   \n",
      "6  Heterogêneo  fold_6  0.968867  0.978166  [[356, 15], [10, 448]]  0.972856   \n",
      "7  Heterogêneo  fold_7  0.977210  0.975983   [[363, 8], [11, 447]]  0.979189   \n",
      "8  Heterogêneo  fold_8  0.977466  0.973799   [[364, 7], [12, 446]]  0.979144   \n",
      "9  Heterogêneo  fold_9  0.962897  0.971616  [[354, 17], [13, 445]]  0.967391   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0       6.702621        0.312598    0.021687        0.988300       0.978313   \n",
      "1       7.039227        0.314576    0.037394        0.987973       0.962606   \n",
      "2       6.270327        0.268211    0.030157        0.987162       0.969843   \n",
      "3       6.625611        0.319103    0.033776        0.988638       0.966224   \n",
      "4       7.063523        0.292224    0.031363        0.989008       0.968637   \n",
      "5       6.990471        0.353294    0.032569        0.988292       0.967431   \n",
      "6       6.685080        0.267891    0.030157        0.988480       0.969843   \n",
      "7       6.987960        0.309688    0.022919        0.986740       0.977081   \n",
      "8       6.886866        0.298370    0.022919        0.988226       0.977081   \n",
      "9       7.277020        0.365051    0.036188        0.988560       0.963812   \n",
      "\n",
      "   precision  \n",
      "0   0.982456  \n",
      "1   0.975446  \n",
      "2   0.973684  \n",
      "3   0.979866  \n",
      "4   0.975717  \n",
      "5   0.973626  \n",
      "6   0.967603  \n",
      "7   0.982418  \n",
      "8   0.984547  \n",
      "9   0.963203  \n",
      "Model ANNs\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "Fold: 5\n",
      "Fold: 6\n",
      "Fold: 7\n",
      "Fold: 8\n",
      "Fold: 9\n",
      "\n",
      " Metrics: \n",
      "  model_name    fold      ACSA    recall                      CM  f1_score  \\\n",
      "0       ANNs  fold_0  0.968253  0.960699   [[363, 9], [18, 440]]  0.970232   \n",
      "1       ANNs  fold_1  0.964021  0.973742  [[355, 17], [12, 445]]  0.968444   \n",
      "2       ANNs  fold_2  0.967554  0.978118  [[356, 16], [10, 447]]  0.971739   \n",
      "3       ANNs  fold_3  0.962333  0.964989  [[357, 15], [16, 441]]  0.966046   \n",
      "4       ANNs  fold_4  0.960645  0.956236  [[359, 13], [20, 437]]  0.963616   \n",
      "5       ANNs  fold_5  0.961225  0.962882  [[356, 15], [17, 441]]  0.964989   \n",
      "6       ANNs  fold_6  0.967264  0.980349   [[354, 17], [9, 449]]  0.971861   \n",
      "7       ANNs  fold_7  0.977398  0.962882   [[368, 3], [17, 441]]  0.977827   \n",
      "8       ANNs  fold_8  0.965269  0.962882  [[359, 12], [17, 441]]  0.968167   \n",
      "9       ANNs  fold_9  0.965592  0.971616  [[356, 15], [13, 445]]  0.969499   \n",
      "\n",
      "   training_time  inference_time  error_rate  train_accuracy  test_accuracy  \\\n",
      "0     105.244484        0.017101    0.032530        0.988300       0.967470   \n",
      "1     109.574575        0.015616    0.034982        0.987030       0.965018   \n",
      "2      99.863786        0.031175    0.031363        0.987411       0.968637   \n",
      "3     133.314834        0.016175    0.037394        0.988013       0.962606   \n",
      "4     132.727945        0.015862    0.039807        0.989008       0.960193   \n",
      "5     112.127796        0.015549    0.038601        0.988056       0.961399   \n",
      "6     104.544101        0.024514    0.031363        0.987854       0.968637   \n",
      "7     113.924228        0.015630    0.024125        0.987115       0.975875   \n",
      "8     112.389386        0.015622    0.034982        0.988352       0.965018   \n",
      "9     116.970819        0.015617    0.033776        0.987852       0.966224   \n",
      "\n",
      "   precision  \n",
      "0   0.979955  \n",
      "1   0.963203  \n",
      "2   0.965443  \n",
      "3   0.967105  \n",
      "4   0.971111  \n",
      "5   0.967105  \n",
      "6   0.963519  \n",
      "7   0.993243  \n",
      "8   0.973510  \n",
      "9   0.967391  \n"
     ]
    }
   ],
   "source": [
    "cv = cross_validation(X_train, y_train, ensembles, use_adasyn=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
